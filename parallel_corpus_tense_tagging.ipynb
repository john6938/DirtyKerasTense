{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verb Tense Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from keras.layers import Embedding, LSTM, GRU, Conv1D, Dense, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Idiap Tense-Annotation Corpus\n",
    "We are going to train our models on a parallel corpus from https://www.idiap.ch/dataset/tense-annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%              \n"
    }
   },
   "outputs": [],
   "source": [
    "def get_verb_info(verb_lines):\n",
    "    verbs_info = dict()\n",
    "    for verb_ln in verb_lines:\n",
    "        verb_data = verb_ln.rstrip('\\t').split('\\t')\n",
    "        verb_positions = verb_data[0].split(\" \")\n",
    "        verb_tense = verb_data[2]\n",
    "        if verb_tense in {'cond other', 'pres other'}:  # we don't want these rare & strange tenses\n",
    "            verb_tense = 'other'\n",
    "        for verb_position in verb_positions:\n",
    "            verbs_info[int(verb_position)] = verb_tense\n",
    "    return verbs_info\n",
    "\n",
    "def get_tagged_tokens(sentence, verbs_information):\n",
    "    tagged_tokens = []\n",
    "    for token_index, token in enumerate(sentence.split(\" \"), 1):\n",
    "        tag = verbs_information[token_index] if token_index in verbs_information else 'O'\n",
    "        tagged_tokens.append((token, tag))\n",
    "    return tagged_tokens\n",
    "\n",
    "def tagged_sentence_generator(ds_path, start_sent_nb=0, end_sent_nb_excl=None):\n",
    "    counter = 0\n",
    "    with open(ds_path, 'r') as f:\n",
    "        sent_data = []\n",
    "        for line in f.readlines():\n",
    "            if line == '\\n' and sent_data:\n",
    "                english_sent, french_sent = sent_data[0], sent_data[1]\n",
    "                verbs_info_dict = get_verb_info(sent_data[2:])\n",
    "                tagged_sentence = get_tagged_tokens(english_sent, verbs_info_dict)\n",
    "                if start_sent_nb <= counter:\n",
    "                    if end_sent_nb_excl is None or counter < end_sent_nb_excl:\n",
    "                        yield tagged_sentence\n",
    "                    else:\n",
    "                        break\n",
    "                counter += 1\n",
    "                sent_data = []\n",
    "            else:\n",
    "                sent_data.append(line.rstrip('\\n'))\n",
    "\n",
    "def get_all_ds_tags_and_len(ds_path):\n",
    "    ds_tags = set()\n",
    "    counter = 0\n",
    "    with open(ds_path, 'r') as f:\n",
    "        sent_data = []\n",
    "        for line in f.readlines():\n",
    "            if line == '\\n' and sent_data:\n",
    "                verbs_info_dict = get_verb_info(sent_data[2:])\n",
    "                ds_tags.update(verbs_info_dict.values())\n",
    "                counter += 1\n",
    "                sent_data = []\n",
    "            else:\n",
    "                sent_data.append(line.rstrip('\\n'))\n",
    "    return ds_tags, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 30 example:  [('Yes', 'O'), (',', 'O'), ('Mrs', 'O'), ('Schroedter', 'O'), (',', 'O'), ('I', 'O'), ('shall', 'fut'), ('be', 'fut'), ('pleased', 'fut'), ('to', 'O'), ('look', 'infinitif'), ('into', 'O'), ('the', 'O'), ('facts', 'O'), ('of', 'O'), ('this', 'O'), ('case', 'O'), ('when', 'O'), ('I', 'O'), ('have', 'pres_perf'), ('received', 'pres_perf'), ('your', 'O'), ('letter', 'O'), ('.', 'O')]\n",
      "Max. sentence length: 858 tokens\n",
      "Number of different tenses: 19, Nb. of sentences in the corpus: 411319\n"
     ]
    }
   ],
   "source": [
    "PATH = \"./CorpusAnnotatedTenseVoice.txt\"\n",
    "SENT_NB = 30            \n",
    "sent_generator = tagged_sentence_generator(PATH, SENT_NB)\n",
    "print(\"Sentence {} example: \".format(SENT_NB), next(sent_generator))\n",
    "sent_generator = tagged_sentence_generator(PATH)\n",
    "max_sent_len = 0\n",
    "for sent in sent_generator:\n",
    "    max_sent_len = max(max_sent_len, len(sent))\n",
    "print(\"Max. sentence length: {} tokens\".format(max_sent_len))\n",
    "ds_tags_set, ds_size = get_all_ds_tags_and_len(PATH)\n",
    "print(\"Number of different tenses: {}, Nb. of sentences in the corpus: {}\".format(len(ds_tags_set), ds_size))\n",
    "ds_tags_set.add('O')  # a default tag for all non-verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Level Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use pre-trained \"GloVe\" word embeddings that can be downloaded from https://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "Note: Words will be kept with their original capitalization. We might want to try to make all words lower-case, too. \"word.lower()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 61760\n",
      "True True\n"
     ]
    }
   ],
   "source": [
    "# create a vocabulary of all words in our dataset\n",
    "words = set([])\n",
    "[[words.add(token) for token, _ in sent] for sent in tagged_sentence_generator(PATH)]\n",
    "print(\"vocabulary size: {}\".format(len(words)))\n",
    "print(\"does\" in words, \"Does\" in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index of `hello`: 32667\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary, an index for each word\n",
    "dictionary = dict()\n",
    "for i, word in enumerate(words):\n",
    "    dictionary[word] = i\n",
    "print(\"index of `hello`: {}\".format(dictionary[\"hello\"]))\n",
    "\n",
    "# a mapping for indexes back into words\n",
    "idx2word = {}\n",
    "for word, i in dictionary.items():\n",
    "    idx2word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'fut_perf',\n",
       " 1: 'other',\n",
       " 2: 'past_perf_cont',\n",
       " 3: 'past_perf',\n",
       " 4: 'cond_cont',\n",
       " 5: 'sim_past other',\n",
       " 6: 'sim_past',\n",
       " 7: 'cond_perf_cont',\n",
       " 8: 'pres_perf',\n",
       " 9: 'past_cont',\n",
       " 10: 'pres',\n",
       " 11: 'pres_cont',\n",
       " 12: 'fut',\n",
       " 13: 'pres_perf_cont',\n",
       " 14: 'cond',\n",
       " 15: 'infinitif',\n",
       " 16: 'cond_perf',\n",
       " 17: 'O',\n",
       " 18: 'fut_perf_cont',\n",
       " 19: 'fut_cont'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert tags to numerical labels, create a dictionary, an index for each token\n",
    "tag2lab = dict()\n",
    "for i, tag in enumerate(ds_tags_set):\n",
    "    tag2lab[tag] = i\n",
    "# a mapping for labels back into tags\n",
    "lab2tag = {}\n",
    "for tag, i in tag2lab.items():\n",
    "    lab2tag[i] = tag\n",
    "lab2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "embedding for the word `word`:\n",
      "[-0.1643     0.15722   -0.55021   -0.3303     0.66463   -0.1152\n",
      " -0.2261    -0.23674   -0.86119    0.24319    0.074499   0.61081\n",
      "  0.73683   -0.35224    0.61346    0.0050975 -0.62538   -0.0050458\n",
      "  0.18392   -0.12214   -0.65973   -0.30673    0.35038    0.75805\n",
      "  1.0183    -1.7424    -1.4277     0.38032    0.37713   -0.74941\n",
      "  2.9401    -0.8097    -0.66901    0.23123   -0.073194  -0.13624\n",
      "  0.24424   -1.0129    -0.24919   -0.06893    0.70231   -0.022177\n",
      " -0.64684    0.59599    0.027092   0.11203    0.61214    0.74339\n",
      "  0.23572   -0.1369   ]\n"
     ]
    }
   ],
   "source": [
    "# let's create a dictionary of embeddings from each word embedding vector in the pre-trained GloVe embeddings file\n",
    "GLOVE_DIR = \".\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "print(\"embedding for the word `word`:\")\n",
    "print(embeddings_index.get(\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# let's try to extract the GloVe embeddings for each word from our dataset vocabulary\n",
    "EMBEDDING_DIM = 50\n",
    "embedding_matrix = np.zeros((len(dictionary) + 1, EMBEDDING_DIM))\n",
    "for word, i in dictionary.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "del embeddings_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking how many words have no pre-trained GloVe word embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of words out of vocabulary: 43.78400259067357 percent\n",
      "examples of words without pre-trained GloVe embeddings:\n",
      "['', 'quick-acting', 'B5-0160', '5178', 'specifically-channelled', 'service-level', 'McDuck', 'Moher', 'Ã“', 'V', 'Head', 'A5-0400', 'fish-catching', 'B5-0846', 'Al-Sabah']\n"
     ]
    }
   ],
   "source": [
    "oov_percentage = 100. * np.count_nonzero(np.all(embedding_matrix == 0, axis=1)) / len(dictionary)  # OOV portion\n",
    "print(\"percentage of words out of vocabulary: %s percent\" % oov_percentage)\n",
    "outta_vocab_idxs = set(np.where(np.all(embedding_matrix == 0, axis=1))[0])\n",
    "outta_vocab_words = [word for word, i in dictionary.items() if i in outta_vocab_idxs]\n",
    "print(\"examples of words without pre-trained GloVe embeddings:\")\n",
    "print(outta_vocab_words[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_batch(x_batch, y_batch, max_sentence_len, nb_labels, tok_dict, tag2label):\n",
    "    # pad sequences with zeros to make them same length: we need it for vectorized computations\n",
    "    x_batch = pad_sequences(x_batch, maxlen=max_sentence_len, padding='post', value=tok_dict[''])            \n",
    "    # convert labels to categorical one-hot vectors\n",
    "    y_batch = pad_sequences(y_batch, maxlen=max_sentence_len, padding='post', value=tag2label['O'])\n",
    "    y_batch = to_categorical(y_batch, nb_labels)\n",
    "    return np.array(x_batch), np.array(y_batch, dtype=\"float64\")\n",
    "\n",
    "def batch_generator(\n",
    "        start_idx, end_idx_excl,\n",
    "        tok_dict, tag2label, nb_labels, ds_path,\n",
    "        max_sentence_len, batch_size=32\n",
    "):\n",
    "    \"\"\"infinitely yield batches of sequences and labels\"\"\"\n",
    "    x_batch, y_batch = [], []\n",
    "    while True:\n",
    "        sentence_generator = tagged_sentence_generator(ds_path, start_idx, end_idx_excl)\n",
    "        for sentence in sentence_generator:\n",
    "            toks, tags = zip(*sentence[:max_sent_len])  # !!! we cut off too long sentences !!!\n",
    "            # convert sentence into sequences of word indexes\n",
    "            sequence = [tok_dict[tok] for tok in toks]  # TODO: we might want to try lower-cased tokens\n",
    "            labels = [tag2label[tag] for tag in tags]\n",
    "            if len(x_batch) == batch_size:\n",
    "                yield prepare_batch(x_batch, y_batch, max_sentence_len, nb_labels, tok_dict, tag2label)\n",
    "                x_batch, y_batch = [], []\n",
    "            x_batch.append(sequence)\n",
    "            y_batch.append(labels)\n",
    "        if len(x_batch) != 0:\n",
    "            yield prepare_batch(x_batch, y_batch, max_sentence_len, nb_labels, tok_dict, tag2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 200  # cutting off too long sentences\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# our dataset will be split into a traing part and a validation part,\n",
    "# where we measure our model's performance during training\n",
    "\n",
    "# we will further keep a testing part to evaluate predictions \n",
    "\n",
    "# TEST_SPLIT = .1\n",
    "# nb_test_samples = int(TEST_SPLIT * ds_size)\n",
    "# print(\"number of validation and test samples: %s\" % nb_test_samples)\n",
    "\n",
    "# train_generator = batch_generator(\n",
    "#     0, ds_size-2*nb_test_samples, dictionary, tag2lab, len(ds_tags_set), PATH, MAX_SEQUENCE_LENGTH, BATCH_SIZE\n",
    "# )\n",
    "# val_generator = batch_generator(\n",
    "#     ds_size-2*nb_test_samples, ds_size-nb_test_samples, dictionary, tag2lab, len(ds_tags_set), PATH, MAX_SEQUENCE_LENGTH, BATCH_SIZE\n",
    "# )\n",
    "# val_generator = batch_generator(\n",
    "#     ds_size-nb_test_samples, ds_size, dictionary, tag2lab, len(ds_tags_set), PATH, MAX_SEQUENCE_LENGTH, BATCH_SIZE\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model\n",
    "Embeddings layer will be using the weights from the pre-trained GloVe vectors. We don't want to change them so we set `trainable=False`,\n",
    "\n",
    "we add a Bidirectional layer of LSTM cells or GRU cells after the Embedding layer,\n",
    "if we set `return_sequences` True, we will get the output of the cells in each timestep of the sequence, that's what we want :)\n",
    "\n",
    "you can change the complexity of the model by setting `HIDDEN_SIZE_LSTM` which changes the number of `units`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 8310s 166ms/step - loss: 0.0154 - accuracy: 0.9961 - val_loss: 0.0090 - val_accuracy: 0.9966\n",
      "Test accuracy: 0.9966673851013184\n"
     ]
    }
   ],
   "source": [
    "train_generator = batch_generator(\n",
    "    0, 50000, dictionary, tag2lab, len(ds_tags_set), PATH, MAX_SEQUENCE_LENGTH, BATCH_SIZE\n",
    ")\n",
    "val_generator = batch_generator(\n",
    "    50000, 55000, dictionary, tag2lab, len(ds_tags_set), PATH, MAX_SEQUENCE_LENGTH, BATCH_SIZE\n",
    ")\n",
    "test_generator = batch_generator(\n",
    "    55000, 60000, dictionary, tag2lab, len(ds_tags_set), PATH, MAX_SEQUENCE_LENGTH, BATCH_SIZE\n",
    ")\n",
    "\n",
    "NB_EPOCHS = 1\n",
    "HIDDEN_SIZE_LSTM = 50\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(dictionary) + 1, output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                    input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model.add(Bidirectional(LSTM(HIDDEN_SIZE_LSTM, return_sequences=True), input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)))\n",
    "model.add(Dense(len(ds_tags_set), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit_generator(train_generator, validation_data=val_generator,\n",
    "                    epochs=NB_EPOCHS, steps_per_epoch=50000, validation_steps=5000)\n",
    "model.save('rnn_model.h5') \n",
    "score, acc = model.evaluate_generator(test_generator, steps=5000, callbacks=None)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predictions(x_test, y_pred, idx2word, lab2tag):\n",
    "    \"\"\"\n",
    "    print the results of our model's predictions after converting them back to tokens and tags\n",
    "    \"\"\"\n",
    "    pad_symbol = ''\n",
    "    for seq, preds in zip(x_test, y_pred):\n",
    "        sentence = []\n",
    "        pad_removed = False\n",
    "        for i in range(len(seq)-1, -1, -1):\n",
    "            word_id, pred = seq[i], preds[i]\n",
    "            word, tag = idx2word[word_id], lab2tag[np.argmax(pred)]\n",
    "            if word == pad_symbol and not pad_removed:\n",
    "                continue\n",
    "            else:\n",
    "                pad_removed = True\n",
    "                sentence.append((word, tag))\n",
    "        print(list(reversed(sentence)))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('-', 'O'), ('(', 'O'), ('SV', 'O'), (')', 'O'), ('Finland', 'O'), ('has', 'pres_perf'), ('been', 'pres_perf'), ('hit', 'pres_perf'), ('by', 'O'), ('serious', 'O'), ('problems', 'O'), ('in', 'O'), ('the', 'O'), ('form', 'O'), ('of', 'O'), ('a', 'O'), ('drastic', 'O'), ('increase', 'O'), ('in', 'O'), ('the', 'O'), ('amount', 'O'), ('of', 'O'), ('beer', 'O'), ('imported', 'other'), ('from', 'O'), ('third', 'O'), ('countries', 'O'), (',', 'O'), ('mainly', 'O'), ('from', 'O'), ('Estonia', 'O'), ('and', 'O'), ('Russia', 'O'), ('.', 'O')]\n",
      "\n",
      "\n",
      "[('It', 'O'), ('is', 'pres'), ('therefore', 'O'), ('a', 'O'), ('very', 'O'), ('positive', 'O'), ('development', 'O'), ('that', 'O'), ('the', 'O'), ('Commission', 'O'), ('is', 'pres_cont'), ('now', 'O'), ('granting', 'pres_cont'), ('Finland', 'O'), ('a', 'O'), ('six-year', 'O'), ('derogation', 'O'), ('to', 'O'), ('enable', 'infinitif'), ('it', 'O'), ('to', 'O'), ('introduce', 'infinitif'), ('a', 'O'), ('new', 'O'), ('limit', 'O'), ('on', 'O'), ('beer', 'O'), ('imports', 'O'), ('from', 'O'), ('third', 'O'), ('countries', 'O'), ('.', 'O')]\n",
      "\n",
      "\n",
      "[('Finland', 'O'), ('is', 'pres'), ('in', 'O'), ('that', 'O'), ('way', 'O'), ('being', 'other'), ('granted', 'other'), ('the', 'O'), ('right', 'O'), ('to', 'O'), ('limit', 'other'), ('beer', 'O'), ('imports', 'O'), ('from', 'O'), ('third', 'O'), ('countries', 'O'), ('to', 'O'), ('six', 'O'), ('litres', 'O'), ('per', 'O'), ('traveller', 'O'), ('per', 'O'), ('day', 'O'), ('up', 'O'), ('until', 'O'), ('1', 'O'), ('January', 'O'), ('2006', 'O'), ('.', 'O')]\n",
      "\n",
      "\n",
      "[('This', 'O'), ('issue', 'O'), ('shows', 'pres'), ('that', 'O'), (',', 'O'), ('in', 'O'), ('the', 'O'), ('case', 'O'), ('of', 'O'), ('Finland', 'O'), (',', 'O'), ('the', 'O'), ('Commission', 'O'), ('realises', 'pres'), ('that', 'O'), ('private', 'O'), ('individuals', 'O'), (\"'\", 'O'), ('being', 'other'), ('more', 'O'), ('or', 'O'), ('less', 'O'), ('free', 'O'), ('to', 'O'), ('import', 'other'), ('alcohol', 'O'), ('across', 'O'), ('national', 'O'), ('borders', 'O'), ('is', 'pres'), ('not', 'O'), ('only', 'O'), ('a', 'O'), ('trade', 'O'), ('and', 'O'), ('tax', 'O'), ('issue', 'O'), ('.', 'O'), ('More', 'O'), ('to', 'O'), ('the', 'O'), ('point', 'O'), (',', 'O'), ('it', 'O'), ('has', 'pres'), ('to', 'O'), ('do', 'infinitif'), ('with', 'O'), ('public', 'O'), ('health', 'O'), ('and', 'O'), ('with', 'O'), ('a', 'O'), ('social', 'O'), ('policy', 'O'), ('on', 'O'), ('alcohol', 'O'), ('aimed', 'other'), ('at', 'O'), ('combating', 'other'), ('the', 'O'), ('horrors', 'O'), ('of', 'O'), ('abuse', 'O'), ('.', 'O')]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's see what our model predicts\n",
    "x_test, _ = next(test_generator)\n",
    "y_pred = model.predict(x_test)\n",
    "print_predictions(x_test[:4], y_pred[:4], idx2word, lab2tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model\n",
    "\n",
    "Use Conv1d instead of RNN layers:\n",
    "\n",
    "`Conv1D(filters=EMBEDDING_DIM, kernel_size=..., activation='relu', padding=...)`\n",
    "\n",
    "We need to preserve the sequence legth, when going from one layer to another, so we set padding='same'\n",
    "\n",
    "kernel_size (window size) is a parameter setting the scope of view for our convolutional filter, how many words we look at.\n",
    "\n",
    "We want a filter for each index of our word embedding vector\n",
    "\n",
    "Try setting padding='causal'. This will make our window (kernel) wider, but we will look only at every other word in a sequence inside the window. This is also called a dilated convolution.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 1413s 28ms/step - loss: 0.0202 - accuracy: 0.9948 - val_loss: 0.0111 - val_accuracy: 0.9953\n",
      "Test accuracy: 0.9954240322113037\n"
     ]
    }
   ],
   "source": [
    "train_generator = batch_generator(\n",
    "    0, 50000, dictionary, tag2lab, len(ds_tags_set), PATH, MAX_SEQUENCE_LENGTH, BATCH_SIZE\n",
    ")\n",
    "val_generator = batch_generator(\n",
    "    50000, 55000, dictionary, tag2lab, len(ds_tags_set), PATH, MAX_SEQUENCE_LENGTH, BATCH_SIZE\n",
    ")\n",
    "test_generator = batch_generator(\n",
    "    55000, 60000, dictionary, tag2lab, len(ds_tags_set), PATH, MAX_SEQUENCE_LENGTH, BATCH_SIZE\n",
    ")\n",
    "\n",
    "NB_EPOCHS = 1\n",
    "WINDOW_SIZES = [5, 5]\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(dictionary) + 1, output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                    input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model.add(Conv1D(filters=EMBEDDING_DIM, kernel_size=WINDOW_SIZES[0], activation='relu', padding=\"causal\"))\n",
    "model.add(Conv1D(filters=EMBEDDING_DIM, kernel_size=WINDOW_SIZES[1], activation='relu', padding=\"same\"))\n",
    "model.add(Dense(len(ds_tags_set), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit_generator(train_generator, validation_data=val_generator,\n",
    "                    epochs=NB_EPOCHS, steps_per_epoch=50000, validation_steps=5000)\n",
    "model.save('cnn_model.h5') \n",
    "score, acc = model.evaluate_generator(test_generator, steps=5000, callbacks=None)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('-', 'O'), ('(', 'O'), ('SV', 'O'), (')', 'O'), ('Finland', 'O'), ('has', 'pres_perf'), ('been', 'pres_perf'), ('hit', 'pres_perf'), ('by', 'O'), ('serious', 'O'), ('problems', 'O'), ('in', 'O'), ('the', 'O'), ('form', 'O'), ('of', 'O'), ('a', 'O'), ('drastic', 'O'), ('increase', 'O'), ('in', 'O'), ('the', 'O'), ('amount', 'O'), ('of', 'O'), ('beer', 'O'), ('imported', 'O'), ('from', 'O'), ('third', 'O'), ('countries', 'O'), (',', 'O'), ('mainly', 'O'), ('from', 'O'), ('Estonia', 'O'), ('and', 'O'), ('Russia', 'O'), ('.', 'O')]\n",
      "\n",
      "\n",
      "[('It', 'O'), ('is', 'pres'), ('therefore', 'O'), ('a', 'O'), ('very', 'O'), ('positive', 'O'), ('development', 'O'), ('that', 'O'), ('the', 'O'), ('Commission', 'O'), ('is', 'pres_cont'), ('now', 'O'), ('granting', 'pres_cont'), ('Finland', 'O'), ('a', 'O'), ('six-year', 'O'), ('derogation', 'O'), ('to', 'O'), ('enable', 'other'), ('it', 'O'), ('to', 'O'), ('introduce', 'infinitif'), ('a', 'O'), ('new', 'O'), ('limit', 'O'), ('on', 'O'), ('beer', 'O'), ('imports', 'O'), ('from', 'O'), ('third', 'O'), ('countries', 'O'), ('.', 'O')]\n",
      "\n",
      "\n",
      "[('Finland', 'O'), ('is', 'pres'), ('in', 'O'), ('that', 'O'), ('way', 'O'), ('being', 'other'), ('granted', 'sim_past'), ('the', 'O'), ('right', 'O'), ('to', 'O'), ('limit', 'other'), ('beer', 'O'), ('imports', 'O'), ('from', 'O'), ('third', 'O'), ('countries', 'O'), ('to', 'O'), ('six', 'O'), ('litres', 'O'), ('per', 'O'), ('traveller', 'O'), ('per', 'O'), ('day', 'O'), ('up', 'other'), ('until', 'O'), ('1', 'O'), ('January', 'O'), ('2006', 'O'), ('.', 'O')]\n",
      "\n",
      "\n",
      "[('This', 'O'), ('issue', 'O'), ('shows', 'pres'), ('that', 'O'), (',', 'O'), ('in', 'O'), ('the', 'O'), ('case', 'O'), ('of', 'O'), ('Finland', 'O'), (',', 'O'), ('the', 'O'), ('Commission', 'O'), ('realises', 'pres'), ('that', 'O'), ('private', 'O'), ('individuals', 'O'), (\"'\", 'O'), ('being', 'pres'), ('more', 'O'), ('or', 'O'), ('less', 'O'), ('free', 'O'), ('to', 'O'), ('import', 'O'), ('alcohol', 'O'), ('across', 'O'), ('national', 'O'), ('borders', 'O'), ('is', 'pres'), ('not', 'O'), ('only', 'O'), ('a', 'O'), ('trade', 'O'), ('and', 'O'), ('tax', 'O'), ('issue', 'O'), ('.', 'O'), ('More', 'O'), ('to', 'O'), ('the', 'O'), ('point', 'O'), (',', 'O'), ('it', 'O'), ('has', 'pres'), ('to', 'O'), ('do', 'infinitif'), ('with', 'O'), ('public', 'O'), ('health', 'O'), ('and', 'O'), ('with', 'O'), ('a', 'O'), ('social', 'O'), ('policy', 'O'), ('on', 'O'), ('alcohol', 'O'), ('aimed', 'other'), ('at', 'O'), ('combating', 'other'), ('the', 'O'), ('horrors', 'O'), ('of', 'O'), ('abuse', 'O'), ('.', 'O')]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's see what our model predicts\n",
    "x_test, _ = next(test_generator)\n",
    "y_pred = model.predict(x_test)\n",
    "print_predictions(x_test[:4], y_pred[:4], idx2word, lab2tag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "widgets": {
   "state": {},
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
